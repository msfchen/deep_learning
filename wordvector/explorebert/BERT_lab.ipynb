{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQUIQGANA96I"
   },
   "source": [
    "# BERT Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hivbLURpB3b2"
   },
   "source": [
    "In 2018, models such as [BERT](https://arxiv.org/abs/1810.04805), [Open GPT](https://openai.com/blog/better-language-models/) and [ELMO](https://arxiv.org/abs/1802.05365) were released and performed well on many benchmark NLP tasks with minimal task-specific tuning. The publicly available pretrained models can be used either to extract high quality language features from your text data, or fine-tuned on a specific task (classification, entity recognition, etc.) with your data to produce [state of the art predictions](https://gluebenchmark.com/leaderboard).\n",
    "\n",
    "In this notebook we will use a pre-trained BERT model to **extract features**, namely word embedding vectors, from text data. We will explore different ways of extracting these embeddings and build a simple binary classifier model to compare different embedding approaches.\n",
    "\n",
    "Traditionally, words were converted to their vector representations by either **uniquely indexing (one-hot encoding)** them or using neural network based word embeddings such as **Word2Vec or Fasttext**. BERT offers an advantage over models like Word2Vec because while each word has a fixed representation in Word2Vec regardless of the context within which the word appears, BERT produces word representations that are dynamically informed by the words around them. For example, given two sentences:\n",
    "\n",
    "<font color='green'>The man was accused of robbing a bank.</font>\n",
    "\n",
    "<font color='green'>The man went fishing by the bank of the river.</font>\n",
    "\n",
    "Models such as Word2Vec would produce the same word embeddings for the word _\"bank\"_ in both sentences (embeddings remain the same regardless of the context), while in BERT word embeddings for _\"bank\"_ would be different for  both sentences. \n",
    "\n",
    "*Note that this is in no way a detailed or a comprehensive explanation on BERT architecture (or its alternatives). It is a brief introduction to BERT and we explore how pre-trained BERT models can be used to extract word embeddings. We will use these embeddings for a simple classification task where we try to predict if a piece of text is a positive or a negative review.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Um30Suwd6uGY"
   },
   "source": [
    "#### This notebook has two sections:\n",
    "I. **BERT Embeddings Walk Through**:\n",
    "    Here we explore how to extract embeddings from a pre-trained BERT model.\n",
    "\n",
    "II. **Classification**:\n",
    "    Here we build a base binary classification model (logisitic regression) for a binary classification task. Your task will be to try improving the model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iy7uS7LPCSWN"
   },
   "source": [
    "## I. BERT Embeddings Walk Through"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4LejUWiiA96K"
   },
   "source": [
    "### Format the input sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lToY1wVAFsyO"
   },
   "source": [
    "BERT expects input sentences to be in a specific format:\n",
    "\n",
    "- Tokens to mark the beginning ([CLS]) and separation/end of sentences ([SEP])\n",
    "- Word piece tokenization\n",
    "- Segment IDs used to distinguish different sentences\n",
    "\n",
    "![bert-input-format](https://i.imgur.com/Z8MXvQG.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bX-gX731vef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.31.1)\n",
      "Collecting boto3 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/73/c1/c25300e0afbe36f550d82affc0d1705076e9ea909ef33e8dd1f7147df10b/boto3-1.12.0-py2.py3-none-any.whl (128kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/87/61/a3d8311dccec246605983a39b074eb175338f21cba774db0163e5ad0a139/regex-2020.1.8-cp37-cp37m-win_amd64.whl (271kB)\n",
      "Collecting sacremoses (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "Collecting tokenizers==0.0.11 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/d3/af5629cf53fac268dadcc69fd4db3096eda17e617ecfa9011787820dd59f/tokenizers-0.0.11-cp37-cp37m-win_amd64.whl (796kB)\n",
      "Collecting sentencepiece (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/c5/e7e2f45c076097ac1a58b21288be25ae4eb4044be899e6c04cd897a00f15/sentencepiece-0.1.85-cp37-cp37m-win_amd64.whl (1.2MB)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.0.10)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.16.2)\n",
      "Collecting botocore<1.16.0,>=1.15.0 (from boto3->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/ba/236f25b9200f0cda4842585205b566979484d38927a8a302cc5c1beea10c/botocore-1.15.0-py2.py3-none-any.whl (5.9MB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n",
      "Collecting joblib (from sacremoses->transformers)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.0->boto3->transformers) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from botocore<1.16.0,>=1.15.0->boto3->transformers) (2.8.0)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\ADMIN\\AppData\\Local\\pip\\Cache\\wheels\\6d\\ec\\1a\\21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, regex, joblib, sacremoses, tokenizers, sentencepiece, transformers\n",
      "Successfully installed boto3-1.12.0 botocore-1.15.0 jmespath-0.9.4 joblib-0.14.1 regex-2020.1.8 s3transfer-0.3.3 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.31.1)\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sklearn) (0.20.3)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\ADMIN\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# Install the required modules\n",
    "!pip install transformers\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yssq0Oi5B4Ui"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97mPwzYKA96L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0216 12:04:36.284759 15268 file_utils.py:38] PyTorch version 1.4.0 available.\n",
      "I0216 12:04:41.748416 15268 filelock.py:274] Lock 3154149138504 acquired on C:\\Users\\ADMIN\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "I0216 12:04:41.749415 15268 file_utils.py:413] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to C:\\Users\\ADMIN\\.cache\\torch\\transformers\\tmpc7lw4759\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5e5bcaf87342d590bd23c9c4f5f414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0216 12:04:42.427872 15268 file_utils.py:423] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0216 12:04:42.429872 15268 file_utils.py:426] creating metadata file for C:\\Users\\ADMIN\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0216 12:04:42.430871 15268 filelock.py:318] Lock 3154149138504 released on C:\\Users\\ADMIN\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
      "I0216 12:04:42.431872 15268 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# We are loading a smaller 'bert-base-uncased' model for this notebook. For more information on pre-trained models, check this: https://github.com/google-research/bert#pre-trained-models\n",
    "import torch\n",
    "from transformers import *\n",
    "\n",
    "# model -> BertModel\n",
    "# tokenizer -> BertTokenizer\n",
    "# model name -> 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDBSw37DA96U"
   },
   "source": [
    "#### Tokenization\n",
    "\n",
    "In the example below, the word \"ephemeral\" is split into smaller subwords and characters. Hash signs preceding these subwords are the tokenizer's way of denoting that this subword or character is part of a larger word and preceded by another subword. \n",
    "\n",
    "The BERT tokenizer was created with a WordPiece model. This model greedily creates a fixed-size vocabulary of individual characters, subwords, and words that best fit our language data. To tokenize a word in this model, the tokenizer first checks if the whole word is in the vocabulary. If not, it tries to break the word into the largest possible subwords contained in the vocabulary, and as a last resort will decompose the word into individual characters. Note that because of this, we don't run into the problem of out of vocabulary words.\n",
    "\n",
    "So, rather than assigning \"ephemeral\" and every other out of vocabulary word to something like 'UNK' (unknown vocabulary token), we split it into subword tokens ['ep', '##hem', '##eral'] that will retain some of the contextual meaning of the original word.\n",
    "\n",
    "\n",
    "(For more information about WordPiece, see the [original paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf) and further disucssion in Google's [Neural Machine Translation System](https://arxiv.org/pdf/1609.08144.pdf).)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-8ZI17NHA96Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ep', '##hem', '##eral']\n"
     ]
    }
   ],
   "source": [
    "# Using the BERT tokenizer\n",
    "example_text = \"ephemeral\"\n",
    "print(tokenizer.tokenize(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ama', '##tor', '##cu', '##list']\n"
     ]
    }
   ],
   "source": [
    "example_text2 = \"amatorculist\"\n",
    "print(tokenizer.tokenize(example_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lv20c8GVA96V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank. [SEP]\n",
      "\n",
      "After tokenization:\n",
      " ['[CLS]', 'after', 'stealing', 'money', 'from', 'the', 'bank', 'vault', ',', 'the', 'bank', 'robber', 'was', 'seen', 'fishing', 'on', 'the', 'mississippi', 'river', 'bank', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Adding tokens [CLS] and [SEP] at start and end of a sentence\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "print (marked_text)\n",
    "\n",
    "# Using BERT word piece tokenizer \n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "print (\"\\nAfter tokenization:\\n\",tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mc_DnkUA96Z"
   },
   "source": [
    "#### Segment IDs\n",
    "\n",
    "BERT is trained on and expects sentence pairs (The [SEP] token is used to separate multiple sentences), using 1s and 0s to distinguish between the two sentences. \n",
    "\n",
    "That is, for each token in \"tokenized_text,\" we must specify which sentence it belongs to: sentence 0 (a series of 0s) or sentence 1 (a series of 1s). For our purposes, single-sentence inputs only require a series of 1s, so we will create a vector of 1s for each token in our input sentence. \n",
    "\n",
    "If you want to process two sentences, assign each word in the first sentence plus the '[SEP]' token a 0, and all tokens of the second sentence a 1.\n",
    "\n",
    "Essentially you can have multiple sentence pairs as shown below:\n",
    "\n",
    "- **\"<font color='green'>[CLS] The man was accused of robbing a bank. [SEP] The man was seen fishing by the river bank. [SEP]</font>\"**\n",
    "  - tokens in this case would be: ['[CLS]', 'the', 'man', 'was', 'accused', 'of', 'robb', '##ing', 'a', 'bank', '.', '[SEP]', 'the', 'man', 'was', 'seen', 'fishing', 'by', 'the', 'river', 'bank', '.', '[SEP]']\n",
    "  - segment ids in this case would be: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "  \n",
    "\n",
    "or you can have it as two separate sentences as shown below:\n",
    "\n",
    "- **\"<font color='green'>[CLS] The man was accused of robbing a bank. [SEP]</font>\"**\n",
    "  - tokens in this case would be: ['[CLS]', 'the', 'man', 'was', 'accused', 'of', 'robb', '##ing', 'a', 'bank', '.', '[SEP]']\n",
    "  - segment ids in this case would be: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "- **\"<font color='green'>[CLS] The man was seen fishing by the river bank. [SEP]</font>\"**\n",
    "  - tokens in this case would be: ['[CLS]', 'the', 'man', 'was', 'seen', 'fishing', 'by', 'the', 'river', 'bank', '.', '[SEP]']\n",
    "  - segment ids in this case would be: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F6Yqic-gA96b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# For our task we're using single sentence per input text\n",
    "segments_ids = [0] * len(tokenized_text)\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hita8_R2A96e"
   },
   "source": [
    "#### Convert tokens to indices\n",
    "\n",
    "We call the tokenizer to match the tokens against their indices in the tokenizer vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RXa99b1UA96f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[CLS]', 101)\n",
      "('after', 2044)\n",
      "('stealing', 11065)\n",
      "('money', 2769)\n",
      "('from', 2013)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('vault', 11632)\n",
      "(',', 1010)\n",
      "('the', 1996)\n",
      "('bank', 2924)\n",
      "('robber', 27307)\n",
      "('was', 2001)\n",
      "('seen', 2464)\n",
      "('fishing', 5645)\n",
      "('on', 2006)\n",
      "('the', 1996)\n",
      "('mississippi', 5900)\n",
      "('river', 2314)\n",
      "('bank', 2924)\n",
      "('.', 1012)\n",
      "('[SEP]', 102)\n"
     ]
    }
   ],
   "source": [
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rEpHYlAZA96i"
   },
   "source": [
    "### Forward pass the input sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lJLcUBF5GOyU"
   },
   "source": [
    "1. The pre-trained BERT PyTorch interface expects the data be in torch tensor format so we first convert the segment_ids and tokenized text \n",
    " \n",
    "2. model.eval() puts our model in evaluation mode as opposed to training mode.\n",
    "\n",
    "3. Calling `from_pretrained` will download the pre-trained model specified. When we load the `bert-base-uncased`, we see the definition of the model printed. This particular model has 13 layers (the forward pass of the model includes the outputs of the initial embedding in addition to the encoder states, so we get 13 layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FkmyKBl1A96j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0216 14:16:23.467798 15268 filelock.py:274] Lock 3154290066824 acquired on C:\\Users\\ADMIN\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
      "I0216 14:16:23.468799 15268 file_utils.py:413] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to C:\\Users\\ADMIN\\.cache\\torch\\transformers\\tmp5ehrrsle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31c2b11950144e4974a77f150f06312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=361, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0216 14:16:23.909138 15268 file_utils.py:423] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0216 14:16:23.911138 15268 file_utils.py:426] creating metadata file for C:\\Users\\ADMIN\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0216 14:16:23.912137 15268 filelock.py:318] Lock 3154290066824 released on C:\\Users\\ADMIN\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685.lock\n",
      "I0216 14:16:23.916139 15268 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0216 14:16:23.917136 15268 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0216 14:16:24.296378 15268 filelock.py:274] Lock 3154297359216 acquired on C:\\Users\\ADMIN\\.cache\\torch\\transformers\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "I0216 14:16:24.297379 15268 file_utils.py:413] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to C:\\Users\\ADMIN\\.cache\\torch\\transformers\\tmpqoz8o5v5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34029dfe29d94e86b524eab784f5fce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0216 14:17:11.001804 15268 file_utils.py:423] storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin in cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0216 14:17:11.002805 15268 file_utils.py:426] creating metadata file for C:\\Users\\ADMIN\\.cache\\torch\\transformers\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0216 14:17:11.004803 15268 filelock.py:318] Lock 3154297359216 released on C:\\Users\\ADMIN\\.cache\\torch\\transformers\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
      "I0216 14:17:11.005806 15268 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states=True)\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ko3otJz5A96n"
   },
   "outputs": [],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers = model(tokens_tensor, token_type_ids=segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yR3Xme5fA96r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 1, 22, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layers x batch_size x sequence length x hidden state dimension\n",
    "n_layers, batch_size, sequence_length, hidden_state_dim = encoded_layers[-1], encoded_layers[0], encoded_layers[0][0], encoded_layers[0][0][0]\n",
    "len(n_layers), len(batch_size), len(sequence_length), len(hidden_state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22, 768])\n",
      "torch.Size([1, 768])\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(encoded_layers[0].shape)\n",
    "print(encoded_layers[1].shape)\n",
    "print(type(encoded_layers[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_layers[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
      "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
      "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
      "         ...,\n",
      "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
      "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n",
      "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]]), tensor([[-0.6031, -0.3342, -0.7174,  0.3347,  0.5145, -0.1722,  0.4502,  0.2768,\n",
      "         -0.3769, -0.9998, -0.3657,  0.7535,  0.9817, -0.0192,  0.7959, -0.3459,\n",
      "         -0.1338, -0.3026,  0.1097,  0.5836,  0.5736,  0.9999,  0.1798,  0.1845,\n",
      "          0.2250,  0.9109, -0.5653,  0.8616,  0.8994,  0.7423, -0.2525,  0.0394,\n",
      "         -0.9894, -0.1331, -0.7763, -0.9826,  0.2223, -0.6115,  0.1941,  0.0177,\n",
      "         -0.7634,  0.2312,  0.9999, -0.7000,  0.4623, -0.2202, -1.0000,  0.1908,\n",
      "         -0.8150,  0.6483,  0.5878,  0.8198,  0.1014,  0.3185,  0.3963, -0.3216,\n",
      "         -0.1701,  0.0588, -0.1544, -0.4987, -0.5284,  0.1228, -0.4823, -0.7788,\n",
      "          0.6954,  0.0891, -0.0855, -0.1500,  0.0390, -0.0760,  0.6154,  0.2662,\n",
      "         -0.0129, -0.7253,  0.1352,  0.2921, -0.5613,  1.0000,  0.1536, -0.9681,\n",
      "          0.7166,  0.2600,  0.4519,  0.5470, -0.2798, -1.0000,  0.3419, -0.2645,\n",
      "         -0.9863,  0.1263,  0.5249, -0.2000,  0.5980,  0.4752, -0.2355, -0.4808,\n",
      "         -0.3786, -0.7284, -0.0909,  0.0124, -0.0689, -0.2531, -0.1324, -0.2361,\n",
      "          0.1732, -0.3216, -0.0188,  0.2302, -0.3221,  0.4996,  0.4346, -0.1935,\n",
      "          0.2968, -0.9292,  0.5326, -0.3695, -0.9876, -0.4770, -0.9902,  0.6349,\n",
      "         -0.1863, -0.2612,  0.9123, -0.1930,  0.3110,  0.0803, -0.7598, -1.0000,\n",
      "          0.0292, -0.0628, -0.1086, -0.2135, -0.9671, -0.9521,  0.3334,  0.8675,\n",
      "          0.2254,  0.9995, -0.2908,  0.9420,  0.0336, -0.4542,  0.4123, -0.4455,\n",
      "          0.4558, -0.4442, -0.0685,  0.3043,  0.0575,  0.1843, -0.6577, -0.3121,\n",
      "         -0.1069, -0.7729, -0.2328,  0.9032, -0.4681, -0.5580,  0.3359, -0.1644,\n",
      "         -0.1572,  0.6441,  0.2810,  0.2651,  0.1532,  0.4485, -0.5299,  0.2616,\n",
      "         -0.7413, -0.0347,  0.2560, -0.2659, -0.6092, -0.9868, -0.2346,  0.4682,\n",
      "          0.9771,  0.5646,  0.2360,  0.4567, -0.2170,  0.1420, -0.9554,  0.9832,\n",
      "         -0.0935,  0.2621, -0.7901,  0.5758, -0.7642, -0.5362,  0.6374, -0.3891,\n",
      "         -0.6368, -0.0045, -0.2658, -0.1721, -0.7466,  0.4832, -0.3128, -0.2640,\n",
      "          0.0435,  0.8879,  0.5961,  0.2972,  0.0742,  0.3041, -0.7367, -0.2117,\n",
      "         -0.0500,  0.0825,  0.0273,  0.9870, -0.5612, -0.0353, -0.8011, -0.9833,\n",
      "         -0.1874, -0.7143,  0.0476, -0.4776,  0.4406, -0.7087, -0.3319,  0.1035,\n",
      "         -0.2959, -0.6915,  0.2840, -0.5485,  0.3292, -0.3018,  0.8687,  0.7867,\n",
      "         -0.5158, -0.2411,  0.9175, -0.7601, -0.6967, -0.0763, -0.1411,  0.6739,\n",
      "         -0.6006,  0.9645,  0.6723,  0.3451, -0.9112, -0.6897, -0.3042, -0.0059,\n",
      "         -0.0918, -0.6402,  0.5602,  0.3850,  0.2737,  0.7868, -0.4155,  0.8343,\n",
      "         -0.9288, -0.9377, -0.9803,  0.1936, -0.9873,  0.7978,  0.1702,  0.6547,\n",
      "         -0.3664, -0.3837, -0.9602,  0.2690,  0.0832,  0.8077, -0.5984, -0.5181,\n",
      "         -0.4783, -0.9236, -0.0773, -0.0542,  0.1906,  0.0659, -0.8882,  0.3529,\n",
      "          0.5142,  0.4257, -0.8504,  0.9686,  1.0000,  0.9745,  0.7975,  0.3701,\n",
      "         -0.9993, -0.9159,  0.9999, -0.9649, -1.0000, -0.8484, -0.5494,  0.2254,\n",
      "         -1.0000, -0.0892,  0.0449, -0.8989,  0.2432,  0.9673,  0.7904, -1.0000,\n",
      "          0.8539,  0.8224, -0.5027,  0.8026, -0.2741,  0.9723,  0.3954,  0.5613,\n",
      "         -0.2778,  0.4943, -0.7842, -0.5614, -0.4786, -0.7206,  0.9944, -0.0361,\n",
      "         -0.2827, -0.8623,  0.6206, -0.0204, -0.2777, -0.9243, -0.2374,  0.5576,\n",
      "          0.4833,  0.1621,  0.2181, -0.4744,  0.1432, -0.0688, -0.3646,  0.5594,\n",
      "         -0.8221, -0.1388,  0.5845, -0.0662,  0.0617, -0.9573,  0.9249, -0.4059,\n",
      "          0.5819,  1.0000,  0.8847, -0.6462,  0.4425,  0.1442,  0.2397,  1.0000,\n",
      "          0.3757, -0.9790, -0.5021,  0.5227, -0.4537, -0.4564,  0.9975, -0.1468,\n",
      "         -0.4530, -0.2211,  0.9849, -0.9892,  0.9874, -0.6354, -0.9533,  0.9617,\n",
      "          0.9100, -0.3272, -0.5809,  0.1221,  0.2137,  0.1775, -0.6459,  0.5595,\n",
      "          0.3555,  0.0030,  0.7183,  0.1490, -0.4345,  0.2424, -0.5088,  0.0014,\n",
      "          0.9025,  0.3578, -0.0445, -0.0357, -0.2723, -0.8130, -0.9344,  0.4227,\n",
      "          1.0000, -0.1721,  0.8243,  0.0665,  0.0115, -0.1497,  0.4186,  0.3168,\n",
      "         -0.3061, -0.5515,  0.7428, -0.7339, -0.9949,  0.1418,  0.0288,  0.1303,\n",
      "          0.9994,  0.6172,  0.1452,  0.4381,  0.9631, -0.0898, -0.1024,  0.3720,\n",
      "          0.9655, -0.2082,  0.4160,  0.2856, -0.4470, -0.0806, -0.5101, -0.1494,\n",
      "         -0.8861,  0.3496, -0.9617,  0.9112,  0.9032,  0.4198,  0.1110,  0.5989,\n",
      "          1.0000, -0.9781,  0.0478,  0.8043, -0.0370, -0.9994, -0.4571, -0.3764,\n",
      "         -0.0341, -0.3013,  0.0021,  0.1011, -0.9597,  0.1910,  0.7922, -0.5135,\n",
      "         -0.9862,  0.0285,  0.4023,  0.2365, -0.9728, -0.3717, -0.4726,  0.2997,\n",
      "         -0.0355, -0.9264,  0.3197, -0.3812,  0.3245, -0.1921,  0.4575,  0.3563,\n",
      "          0.9515, -0.8870, -0.3251, -0.1148, -0.6957,  0.4810, -0.2975, -0.7607,\n",
      "         -0.1648,  1.0000, -0.4754,  0.5410,  0.3828,  0.1635, -0.2463,  0.1942,\n",
      "          0.8299,  0.1894,  0.0876, -0.6173,  0.7333, -0.2475,  0.3843,  0.7312,\n",
      "         -0.0760,  0.6759,  0.7408,  0.1733,  0.0710,  0.0930,  0.9232, -0.0188,\n",
      "         -0.2704, -0.3332, -0.0659, -0.3261,  0.7431,  1.0000,  0.1622,  0.5590,\n",
      "         -0.9939, -0.7758, -0.7434,  1.0000,  0.8649, -0.6865,  0.5643,  0.4938,\n",
      "         -0.2507, -0.0745, -0.1468, -0.2566,  0.2531,  0.0162,  0.9603, -0.5751,\n",
      "         -0.9828, -0.4233,  0.3365, -0.9327,  0.9997, -0.5279, -0.2036, -0.2868,\n",
      "         -0.4147, -0.9348, -0.0477, -0.9812, -0.1836,  0.1210,  0.9609,  0.2963,\n",
      "         -0.4743, -0.7886,  0.8539,  0.4805, -0.8080, -0.9297,  0.9734, -0.8989,\n",
      "          0.3318,  1.0000,  0.4983,  0.0046,  0.1519, -0.2174,  0.3163, -0.3975,\n",
      "          0.5479, -0.9144, -0.0836, -0.1866,  0.3943, -0.1005, -0.9010,  0.5822,\n",
      "          0.0541, -0.4141, -0.4825, -0.0623,  0.3400,  0.6302, -0.1646, -0.0293,\n",
      "          0.1773,  0.0035, -0.7814, -0.2836, -0.3813, -0.9999,  0.5009, -1.0000,\n",
      "          0.6917, -0.3492, -0.2238,  0.7105,  0.8107,  0.7481, -0.4549, -0.4793,\n",
      "          0.7201,  0.6103, -0.1528, -0.2139, -0.4457,  0.2485,  0.0196,  0.1607,\n",
      "         -0.3812,  0.5145, -0.2807,  1.0000,  0.0829, -0.3200, -0.5929,  0.2053,\n",
      "         -0.2679,  1.0000, -0.2241, -0.9651,  0.1681, -0.5537, -0.5055,  0.4764,\n",
      "         -0.0439, -0.7710, -0.8442,  0.7623,  0.3223, -0.5828,  0.5065, -0.2420,\n",
      "         -0.2434, -0.0287,  0.8268,  0.9844,  0.8406,  0.3518, -0.9611, -0.3503,\n",
      "          0.9309,  0.1013, -0.1565, -0.0012,  1.0000,  0.4139, -0.7002,  0.0942,\n",
      "         -0.8437, -0.2698, -0.7333,  0.2633,  0.0600,  0.9015, -0.2510,  0.9170,\n",
      "         -0.7876, -0.0735, -0.5044,  0.2206,  0.2786, -0.8753, -0.9837, -0.9885,\n",
      "          0.5545, -0.2806, -0.0823,  0.3698,  0.1557,  0.2687,  0.4354, -1.0000,\n",
      "          0.9126,  0.3169,  0.7036,  0.9720,  0.4816,  0.6401,  0.3309, -0.9813,\n",
      "         -0.6150, -0.3007, -0.1601,  0.3913,  0.3273,  0.6264,  0.2662, -0.3612,\n",
      "         -0.6613, -0.3084, -0.9703, -0.9852,  0.3886,  0.0251, -0.3822,  0.9373,\n",
      "         -0.0961,  0.0824,  0.4406, -0.5929,  0.1701,  0.6097,  0.1687, -0.0525,\n",
      "          0.5533,  0.8141,  0.7725,  0.9789, -0.7193,  0.2995, -0.5652,  0.3757,\n",
      "          0.9063, -0.9190,  0.0740,  0.3566, -0.1535,  0.1921, -0.1948, -0.4410,\n",
      "          0.9270, -0.0852,  0.3333, -0.2275,  0.1727, -0.3694, -0.1233, -0.7592,\n",
      "         -0.5162,  0.4856, -0.1530,  0.8161,  0.7259, -0.0103, -0.4483, -0.1682,\n",
      "          0.0424, -0.8891,  0.2156, -0.0442,  0.6451,  0.3764, -0.4232,  0.9787,\n",
      "         -0.0989, -0.3853, -0.3173, -0.5348,  0.5307, -0.6883, -0.3786, -0.4665,\n",
      "          0.7207,  0.3239,  0.9999, -0.5526, -0.4825, -0.3439, -0.3580,  0.1069,\n",
      "         -0.2620, -1.0000,  0.2673, -0.3037,  0.4706, -0.6132,  0.8511, -0.4910,\n",
      "         -0.7231, -0.1092,  0.5412,  0.5265, -0.4635, -0.2461,  0.4946, -0.3469,\n",
      "          0.9149,  0.5902, -0.1862,  0.7004,  0.5465, -0.2839, -0.5723,  0.6648]]), (tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
      "         [ 0.2329,  0.1390,  0.2979,  ..., -0.0655,  0.8885,  0.5109],\n",
      "         [ 0.2257, -0.7165, -0.7255,  ...,  0.4844,  0.6030, -0.0957],\n",
      "         ...,\n",
      "         [-0.0374, -0.6155, -1.4419,  ...,  0.0793, -0.0811, -0.3802],\n",
      "         [-0.0228,  0.4207, -0.3288,  ...,  0.4464,  0.5178,  0.5501],\n",
      "         [-0.2350,  0.1566, -0.0462,  ..., -0.4206,  0.3074, -0.2288]]]), tensor([[[ 0.0522,  0.0595, -0.2179,  ...,  0.2280, -0.0712,  0.0148],\n",
      "         [ 0.3819,  0.1475,  0.2414,  ...,  0.3397,  0.7607,  0.4999],\n",
      "         [ 0.1705, -0.6168, -0.7296,  ...,  0.8631,  0.6274, -0.3727],\n",
      "         ...,\n",
      "         [ 0.6982, -0.4554, -1.7845,  ...,  0.3308,  0.0710, -0.5187],\n",
      "         [-0.0905,  0.1862, -0.4437,  ...,  0.2244,  0.1810,  0.3740],\n",
      "         [-0.0825,  0.0466, -0.1526,  ..., -0.2033,  0.3370, -0.1767]]]), tensor([[[-0.0357, -0.2022, -0.4103,  ...,  0.2511,  0.0586, -0.0547],\n",
      "         [ 0.1430,  0.0747,  0.0595,  ..., -0.2914,  0.1733,  0.4265],\n",
      "         [ 0.5752, -1.2385, -0.5650,  ...,  0.8995,  0.4652, -0.8080],\n",
      "         ...,\n",
      "         [ 0.9157, -0.4118, -1.6042,  ...,  0.1454,  0.1699, -0.2230],\n",
      "         [-0.1816,  0.0099,  0.0370,  ..., -0.2178,  0.0481,  0.3477],\n",
      "         [-0.1524, -0.1068, -0.0868,  ..., -0.1369,  0.2633, -0.3754]]]), tensor([[[-0.0183, -0.3853, -0.1600,  ...,  0.2446,  0.2160,  0.0633],\n",
      "         [ 0.7727, -0.3786,  0.6677,  ..., -0.2014,  0.0094,  0.6461],\n",
      "         [ 0.8822, -0.7807, -0.6305,  ...,  0.5223,  0.3687, -0.9875],\n",
      "         ...,\n",
      "         [ 0.9788, -0.0669, -1.6344,  ...,  0.0341,  0.0437, -0.1636],\n",
      "         [-0.4724, -0.0258,  0.3107,  ..., -0.2553, -0.1049,  0.0897],\n",
      "         [-0.0816, -0.1028,  0.0947,  ...,  0.0091,  0.0649, -0.0646]]]), tensor([[[ 0.0207, -0.3990, -0.8255,  ...,  0.3448,  0.0951,  0.3650],\n",
      "         [ 0.5667, -0.7513,  0.0695,  ..., -0.1052, -0.1700,  0.4333],\n",
      "         [ 0.7095, -0.2044, -0.1709,  ...,  0.7092,  0.0542, -1.0380],\n",
      "         ...,\n",
      "         [ 1.3519,  0.3352, -1.3555,  ...,  0.1676, -0.2817,  0.1042],\n",
      "         [-0.2488,  0.1044, -0.1778,  ..., -0.2713, -0.2643,  0.2310],\n",
      "         [-0.0266, -0.0469,  0.0053,  ...,  0.0085,  0.0417, -0.0486]]]), tensor([[[-0.3443, -0.6315, -0.6647,  ...,  0.0050,  0.1167,  0.3614],\n",
      "         [ 0.8173, -0.9999, -0.1752,  ..., -0.4932, -0.2556,  0.2038],\n",
      "         [ 0.8973, -0.2652, -0.6559,  ...,  0.5725,  0.4178, -0.6356],\n",
      "         ...,\n",
      "         [ 1.5582,  0.4479, -1.2054,  ...,  0.2052, -0.5083,  0.3994],\n",
      "         [-0.1898,  0.0898, -0.2766,  ..., -0.3044, -0.4370,  0.4248],\n",
      "         [-0.0264, -0.0364,  0.0304,  ...,  0.0160,  0.0067, -0.0486]]]), tensor([[[-2.3737e-01, -9.9507e-01, -4.5364e-01,  ..., -2.1456e-01,\n",
      "           3.5440e-01,  2.5447e-01],\n",
      "         [ 5.7495e-01, -9.1262e-01, -2.3175e-01,  ..., -1.8444e-01,\n",
      "          -1.8389e-01,  1.3615e-01],\n",
      "         [ 2.5019e-01, -5.6334e-01, -7.0052e-01,  ...,  2.0831e-01,\n",
      "           6.6902e-01,  5.5973e-02],\n",
      "         ...,\n",
      "         [ 1.5415e+00,  3.5879e-03, -1.0563e+00,  ...,  6.0116e-02,\n",
      "          -3.6085e-01,  5.3854e-01],\n",
      "         [-5.8238e-01,  8.6807e-02, -4.4414e-01,  ..., -8.1630e-01,\n",
      "          -4.2633e-01,  4.1039e-01],\n",
      "         [ 1.3974e-02, -5.6850e-02, -4.0642e-03,  ...,  7.8786e-05,\n",
      "          -1.4510e-02, -4.7457e-02]]]), tensor([[[-1.5213e-01, -9.7455e-01, -5.6574e-01,  ..., -6.0023e-01,\n",
      "           3.0606e-01,  3.6930e-01],\n",
      "         [ 2.7012e-01, -5.4540e-01,  1.6653e-02,  ...,  6.2675e-02,\n",
      "           2.6824e-01,  3.4130e-02],\n",
      "         [-4.1255e-01, -6.4567e-01, -2.0088e-01,  ...,  2.3458e-01,\n",
      "           1.4713e-01,  4.0928e-03],\n",
      "         ...,\n",
      "         [ 1.3217e+00, -6.6627e-02, -8.1355e-01,  ..., -3.9645e-01,\n",
      "          -5.2720e-01,  2.0277e-01],\n",
      "         [-5.6073e-01, -4.5253e-01, -5.8201e-01,  ..., -1.4335e+00,\n",
      "           2.8755e-01,  5.9470e-01],\n",
      "         [ 6.2797e-05, -2.2453e-02, -2.1868e-02,  ..., -1.4897e-02,\n",
      "           2.8921e-03, -4.3436e-02]]]), tensor([[[ 0.0967, -0.8567, -0.5056,  ..., -0.4204,  0.1267,  0.3225],\n",
      "         [-0.0109, -0.2976,  0.2986,  ..., -0.3344,  0.2755, -0.1571],\n",
      "         [-0.6171, -0.4909, -0.1630,  ..., -0.3340,  0.4452, -0.2641],\n",
      "         ...,\n",
      "         [ 0.9738,  0.0119, -0.6562,  ..., -0.3114, -0.1033,  0.1980],\n",
      "         [-0.8265, -0.4334, -0.7586,  ..., -1.2721,  0.0482,  0.3751],\n",
      "         [ 0.0196,  0.0017,  0.0323,  ..., -0.0348, -0.0410, -0.0743]]]), tensor([[[-0.3495, -0.7788, -0.7701,  ..., -0.0715,  0.2350,  0.1686],\n",
      "         [-0.2694, -0.3884, -0.3181,  ..., -0.7334,  0.0196, -0.4148],\n",
      "         [-0.5444, -0.3546, -0.2131,  ..., -0.5306,  0.2799, -0.3587],\n",
      "         ...,\n",
      "         [ 0.6356,  0.0980, -0.2775,  ..., -0.6243, -0.2620,  0.1343],\n",
      "         [-0.4167,  0.1132, -0.4761,  ..., -0.5415, -0.1268,  0.0483],\n",
      "         [-0.0671, -0.0451,  0.0232,  ..., -0.0718, -0.0020, -0.0556]]]), tensor([[[-0.9560, -1.0372, -0.8875,  ...,  0.0905, -0.3867,  0.3258],\n",
      "         [-0.3215, -1.1213, -0.2349,  ..., -0.5940,  0.1851, -0.5076],\n",
      "         [-0.4795, -1.0745, -0.1744,  ..., -0.5026,  0.1293, -0.1963],\n",
      "         ...,\n",
      "         [ 0.2736, -0.4021, -0.1525,  ..., -0.7099, -0.7498, -0.0066],\n",
      "         [-0.0142,  0.0187, -0.0490,  ..., -0.0197, -0.0281,  0.0165],\n",
      "         [-0.5765, -0.5547, -0.2122,  ...,  0.2610, -0.4111, -0.1223]]]), tensor([[[-0.6121, -0.6371, -0.8917,  ...,  0.1026, -0.2241,  0.3330],\n",
      "         [-0.2817, -0.6142, -0.4499,  ..., -0.7273,  0.0304, -0.5106],\n",
      "         [-0.4519, -0.2628, -0.1917,  ..., -0.5405, -0.2146, -0.2140],\n",
      "         ...,\n",
      "         [ 0.3213, -0.2997, -0.0471,  ..., -0.8094, -0.7861, -0.0618],\n",
      "         [ 0.0430,  0.0219, -0.0214,  ...,  0.0196, -0.0353,  0.0072],\n",
      "         [-0.0599, -0.6397, -0.6008,  ...,  0.4218, -0.5170, -0.1616]]]), tensor([[[-0.4964, -0.1831, -0.5231,  ..., -0.1902,  0.3738,  0.3964],\n",
      "         [-0.1323, -0.2762, -0.3495,  ..., -0.4567,  0.3786, -0.1096],\n",
      "         [-0.3626, -0.4002,  0.0676,  ..., -0.3207, -0.2709, -0.3004],\n",
      "         ...,\n",
      "         [ 0.2961, -0.2856, -0.0382,  ..., -0.6056, -0.5163,  0.2005],\n",
      "         [ 0.4878, -0.0909, -0.2358,  ..., -0.0017, -0.5945, -0.2431],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         [-0.2517, -0.3519, -0.4688,  ...,  0.2500,  0.0336, -0.2627]]])))\n"
     ]
    }
   ],
   "source": [
    "# print(type(encoded_layers)) \n",
    "# <class 'tuple'>\n",
    "print(encoded_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5ssRnKeA96u"
   },
   "source": [
    "#### BERT output\n",
    "\n",
    "The `encoded_layers` variable in the section above has four dimensions:\n",
    "\n",
    "1. The layer number (13 layers)\n",
    "2. The batch number (1 sentence)\n",
    "3. The word / token number (number of tokens in our sentence)\n",
    "4. Hidden state for that layer (768 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qqNlWUjbA96v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in sequence: 22\n",
      "Number of layers per token: 12\n"
     ]
    }
   ],
   "source": [
    "# Convert the hidden state embeddings into single token vectors\n",
    "\n",
    "# Will have the shape: [# tokens, # layers, # features]\n",
    "token_embeddings = [] \n",
    "\n",
    "batch_i = 0\n",
    "# For each token in the sentence...\n",
    "for token_i in range(len(tokenized_text)):\n",
    "  \n",
    "    # Holds 12 layers of hidden states for each token \n",
    "    hidden_layers = [] \n",
    "\n",
    "    # For each of the 12 layers...[last layer is excluded. See issue : https://github.com/huggingface/transformers/issues/1332]\n",
    "    for layer_i in range(len(encoded_layers[-1][0:12])):\n",
    "\n",
    "        # Lookup the vector for `token_i` in `layer_i`\n",
    "        vec = encoded_layers[-1][layer_i][batch_i][token_i]\n",
    "\n",
    "        hidden_layers.append(vec)\n",
    "    \n",
    "    token_embeddings.append(hidden_layers)\n",
    "\n",
    "# Sanity check the dimensions:\n",
    "print (\"Number of tokens in sequence:\", len(token_embeddings))\n",
    "print (\"Number of layers per token:\", len(token_embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qI1OBkNjA96y"
   },
   "source": [
    "### Create word and sentence embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ytonVXQWGcei"
   },
   "source": [
    "To extract features we'd like individual vectors for each of our tokens, or even a single vector representation of the whole sentence.\n",
    "\n",
    "In order to get the individual vectors we can combine some of the layer vectors. BERT authors tested different combinations of this by feeding different vector combinations as input features to a BiLSTM used on a named entity recognition task and observing the resulting F1 scores.\n",
    "\n",
    "![bert-feature-extraction-contextualized-embeddings](https://jalammar.github.io/images/bert-feature-extraction-contextualized-embeddings.png)\n",
    "\n",
    "Using the last four layers produced the best results on this specific task. Many of the other methods returned close results, and so it is advisable to test different versions depending on the specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HR2ENfjA96z"
   },
   "source": [
    "#### Word vectors\n",
    "\n",
    "Let's create word vectors by **using just the last layer**, to give us a single vector per token. Each of those vectors will have a length of `768`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oLU97qgOA960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of token_vecs: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "# Every token vector : [sentence sequence length x 768]\n",
    "token_vecs = [each_token[-1] for each_token in token_embeddings]\n",
    "\n",
    "print('shape of token_vecs: %d x %d' % (len(token_vecs), len(token_vecs[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZDba-lymA964"
   },
   "source": [
    "Another way of creating word vectors is by **summing** together the last four layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKIZ-_ZeA966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [sentence sequence length x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(torch.stack(token)[-4:], 0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPBbx0KCA96-"
   },
   "source": [
    "#### Sentence vectors\n",
    "To get a single vector for our entire sentence we have multiple application-dependent strategies, but a simple approach is to average the last hidden layer of each token, producing a single 768 length vector. \n",
    "<a href=\"https://stackoverflow.com/questions/29760935/how-to-get-vector-for-a-sentence-from-the-word2vec-of-tokens-in-sentence#answer-31738627\">Here</a> are different ways of creating sentence vectors from word vectors (the link refers to Word2Vec embeddings, however the approach can be used here as well). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iFwwn25IA96_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sentence embedding [1, 768]\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding = torch.mean(encoded_layers[-1][11], 1)\n",
    "print(\"shape of sentence embedding\", list(sentence_embedding.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpRnRCl5A97D"
   },
   "source": [
    "### Confirming contextually dependent vectors\n",
    "\n",
    "To confirm that the value of these vectors are in fact contextually dependent, let's take a look at the output from the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1xIrelVA97F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stealing money from the bank vault, the bank robber was seen fishing on the Mississippi river bank.\n"
     ]
    }
   ],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zdjvu57AA97J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(tokenized_text):\n",
    "    print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t6hXhbvkA97M"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"river bank\"\n",
    "different_bank = cosine_similarity(token_vecs_sum[10].reshape(1,-1), token_vecs_sum[19].reshape(1,-1))[0][0]\n",
    "\n",
    "# Compare \"bank\" as in \"bank robber\" to \"bank\" as in \"bank vault\" \n",
    "same_bank = cosine_similarity(token_vecs_sum[10].reshape(1,-1), token_vecs_sum[6].reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7XVD3P5A97O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'bank' as in 'bank robber' to 'bank' as in 'bank vault': 0.92893326\n"
     ]
    }
   ],
   "source": [
    "print (\"Similarity of 'bank' as in 'bank robber' to 'bank' as in 'bank vault':\",  same_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I_qFFA2IA97S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity of 'bank' as in 'bank robber' to 'bank' as in 'river bank': 0.6819416\n"
     ]
    }
   ],
   "source": [
    "print (\"Similarity of 'bank' as in 'bank robber' to 'bank' as in 'river bank':\",  different_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q674wxL94PvF"
   },
   "source": [
    "#### <font color='red'>Try for yourself</font>\n",
    "\n",
    "Can you find one more example where the similarity of word embeddings for the same word in a similar context (such as 'bank' in 'bank robber' and 'bank' in 'bank vault') is greater than the similarity of word embeddings for same words in a different context (such as 'bank' in 'bank robber' and 'river bank')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qmz8eTK1A97Y"
   },
   "source": [
    "## II. Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tdM1nNpA97b"
   },
   "source": [
    "Next we'll use the `bert-base-uncased` pre-trained model and test the embeddings on a classification task. For this task we'll be using a food reviews dataset and build a small classification model to classify reviews as either positive or negative. \n",
    "\n",
    "For this exercise we take a look at two ways of creating word embeddings:\n",
    "1. **Summing** together the hidden state of the last four layers. \n",
    "2. Using the hidden state of the last layer only.\n",
    "\n",
    "Both of these methods would create a word embedding of shape 768. One simple way of creating **sentence embeddings** is to extract word vectors for each of the tokens in a given sentence and calculate the mean vector of all the token embeddings. This is how we'll be creating our sentence embeddings for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lg3_P58FSgjQ"
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFb6lDf0Rx5k"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brgE_KeXvbKh"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(C, class_labels=['0', '1']):\n",
    "    \"\"\"\n",
    "    :C: shape (2,2) as given by scikit-learn confusion_matrix function ['ndarray']\n",
    "    :class_labels: list of strings, default simply labels 0 and 1 ['List']\n",
    "\n",
    "    Draws confusion matrix with associated metrics.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    assert C.shape == (2, 2), \"Confusion matrix should be from binary classification only.\"\n",
    "\n",
    "    # true negative, false positive, etc...\n",
    "    tn = C[0, 0]\n",
    "    fp = C[0, 1]\n",
    "    fn = C[1, 0]\n",
    "    tp = C[1, 1]\n",
    "\n",
    "    NP = fn + tp  # Num positive examples\n",
    "    NN = tn + fp  # Num negative examples\n",
    "    N = NP + NN\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(C, interpolation='nearest', cmap=plt.cm.gray)\n",
    "\n",
    "    # Draw the grid boxes\n",
    "    ax.set_xlim(-0.5, 2.5)\n",
    "    ax.set_ylim(2.5, -0.5)\n",
    "    ax.plot([-0.5, 2.5], [0.5, 0.5], '-k', lw=2)\n",
    "    ax.plot([-0.5, 2.5], [1.5, 1.5], '-k', lw=2)\n",
    "    ax.plot([0.5, 0.5], [-0.5, 2.5], '-k', lw=2)\n",
    "    ax.plot([1.5, 1.5], [-0.5, 2.5], '-k', lw=2)\n",
    "\n",
    "    # Set xlabels\n",
    "    ax.set_xlabel('Predicted Label', fontsize=16)\n",
    "    ax.set_xticks([0, 1, 2])\n",
    "    ax.set_xticklabels(class_labels + [''])\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.xaxis.tick_top()\n",
    "    # These coordinate might require some tinkering. Ditto for y, below.\n",
    "    ax.xaxis.set_label_coords(0.34, 1.06)\n",
    "\n",
    "    # Set ylabels\n",
    "    ax.set_ylabel('True Label', fontsize=16, rotation=90)\n",
    "    ax.set_yticklabels(class_labels + [''], rotation=90)\n",
    "    ax.set_yticks([0, 1, 2])\n",
    "    ax.yaxis.set_label_coords(-0.09, 0.65)\n",
    "\n",
    "    # Fill in initial metrics: tp, tn, etc...\n",
    "    ax.text(0, 0,\n",
    "            'True Neg: %d\\n(Num Neg: %d)' % (tn, NN),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(0, 1,\n",
    "            'False Neg: %d' % fn,\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1, 0,\n",
    "            'False Pos: %d' % fp,\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1, 1,\n",
    "            'True Pos: %d\\n(Num Pos: %d)' % (tp, NP),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    # Fill in secondary metrics: accuracy, true pos rate, etc...\n",
    "    ax.text(2, 0,\n",
    "            'False Pos Rate: %.2f' % (fp / (fp + tn + 0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(2, 1,\n",
    "            'True Pos Rate: %.2f' % (tp / (tp + fn + 0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(2, 2,\n",
    "            'Accuracy: %.2f' % ((tp + tn + 0.) / N),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(0, 2,\n",
    "            'Neg Pre Val: %.2f' % (1 - fn / (fn + tn + 0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    ax.text(1, 2,\n",
    "            'Pos Pred Val: %.2f' % (tp / (tp + fp + 0.)),\n",
    "            va='center',\n",
    "            ha='center',\n",
    "            bbox=dict(fc='w', boxstyle='round,pad=1'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4OFR1hAV99U"
   },
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    LAST_LAYER = 1\n",
    "    LAST_4_LAYERS = 2\n",
    "    def __init__(self):\n",
    "        self._tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self._bert_model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "        self._bert_model.eval()\n",
    "\n",
    "    def tokenize(self, sentence):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sentence: input sentence ['str']\n",
    "        :return: tokenized sentence based on word piece model ['List']\n",
    "        \"\"\"\n",
    "        marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "        tokenized_text = self._tokenizer.tokenize(marked_sentence)\n",
    "        return tokenized_text\n",
    "\n",
    "    def get_bert_embeddings(self, sentence):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sentence: input sentence ['str']\n",
    "        :return: BERT pre-trained hidden states (list of torch tensors) ['List']\n",
    "        \"\"\"\n",
    "        # Predict hidden states features for each layer\n",
    "\n",
    "        tokenized_text = self.tokenize(sentence)\n",
    "        indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        segments_ids = [0] * len(tokenized_text)\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoded_layers = self._bert_model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "\n",
    "        return encoded_layers[-1][0:12]\n",
    "\n",
    "    def sentence2vec(self, sentence, layers):\n",
    "        \"\"\"\n",
    "\n",
    "        :param sentence: input sentence ['str']\n",
    "        :param layers: parameter to decide how word embeddings are obtained ['str]\n",
    "            1. 'last' : last hidden state used to obtain word embeddings for sentence tokens\n",
    "            2. 'last_4' : last 4 hidden states used to obtain word embeddings for sentence tokens\n",
    "\n",
    "        :return: sentence vector [List]\n",
    "        \"\"\"\n",
    "        encoded_layers = self.get_bert_embeddings(sentence)\n",
    "        \n",
    "        if layers == 1:\n",
    "            # using the last layer embeddings\n",
    "            token_embeddings = encoded_layers[-1]\n",
    "            # summing the last layer vectors for each token\n",
    "            sentence_embedding = torch.mean(token_embeddings, 1)\n",
    "            return sentence_embedding.view(-1).tolist()\n",
    "\n",
    "        elif layers == 2:\n",
    "            token_embeddings = []\n",
    "            tokenized_text = self.tokenize(sentence)\n",
    "\n",
    "            batch_i = 0\n",
    "            # For each token in the sentence...\n",
    "            for token_i in range(len(tokenized_text)):\n",
    "\n",
    "                # Holds 12 layers of hidden states for each token\n",
    "                hidden_layers = []\n",
    "\n",
    "                # For each of the 12 layers...\n",
    "                for layer_i in range(len(encoded_layers)):\n",
    "                    # Lookup the vector for `token_i` in `layer_i`\n",
    "                    vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "                    hidden_layers.append(list(vec.numpy()))\n",
    "\n",
    "                token_embeddings.append(hidden_layers)\n",
    "\n",
    "            # using the last 4 layer embeddings\n",
    "            token_vecs_sum = []\n",
    "\n",
    "            # For each token in the sentence...\n",
    "            for token in token_embeddings:\n",
    "                # Sum the vectors from the last four layers.\n",
    "                sum_vec = np.sum(token[-4:], axis=0)\n",
    "\n",
    "                # Use `sum_vec` to represent `token`.\n",
    "                token_vecs_sum.append(list(sum_vec))\n",
    "\n",
    "            # summing the last layer vectors for each token\n",
    "            sentence_embedding = np.mean(token_vecs_sum, axis=0)\n",
    "            return sentence_embedding.ravel().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SEbLFCReA97h"
   },
   "outputs": [],
   "source": [
    "def model_grid_search(parameters, x_train, y_train, model, scoring):\n",
    "    \"\"\"\n",
    "\n",
    "    :param parameters: hyperparameter choices to tune. Eg:\n",
    "                    Hyperparameter values for logistic regression\n",
    "                    {'loss' :['log'],\n",
    "                    'penalty':['l1','l2','elasticnet'],\n",
    "                    'alpha':[float(i)/10000000 for i in range(1,5,1)],\n",
    "                    'n_jobs':[-1]}\n",
    "\n",
    "                    [dict]\n",
    "\n",
    "    :param x_train: training data (2D matrix) [List or ndarray]\n",
    "    :param y_train: training data predictor values [List or ndarray]\n",
    "    :param model: sklearn model to perform gridsearch on\n",
    "    :param scoring: Finds the best model based on the scoring metric used (https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
    "    :return: grid search instance with information on all trained classifiers\n",
    "    \"\"\"\n",
    "    clf = GridSearchCV(model, parameters, cv=5, scoring=scoring)\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def test_metrics(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param y_pred: predicted values [List]\n",
    "    :param y_test: actual predictor values [List]\n",
    "    \n",
    "    :return precision, recall, F1 and confusion matrix scores [Dict]\n",
    "    \"\"\"\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Precision and recall\n",
    "    fp = conf_matrix[0, 1]\n",
    "    fn = conf_matrix[1, 0]\n",
    "    tp = conf_matrix[1, 1]\n",
    "\n",
    "    precision = 100 * float(tp) / (tp + fp)\n",
    "    recall = 100 * float(tp) / (tp + fn)\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'F1': F1,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oKpuRelAA97m"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0216 17:48:06.512640 15268 tokenization_utils.py:418] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0216 17:48:06.936713 15268 configuration_utils.py:254] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0216 17:48:06.937711 15268 configuration_utils.py:290] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"do_sample\": false,\n",
      "  \"eos_token_ids\": 0,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0216 17:48:07.370817 15268 modeling_utils.py:458] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at C:\\Users\\ADMIN\\.cache\\torch\\transformers\\aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"
     ]
    }
   ],
   "source": [
    "embed_model = Embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEE6jwRfSgjj"
   },
   "source": [
    "#### Dataset\n",
    "This dataset consists of reviews of fine foods from Amazon. The original dataset spans a period of more than 10 years, including ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories. The `Score` column consists of scores ranging from 1 to 5 where **1** and **2** represent negative reviews, **4** and **5** represent positive reviews and a score of **3** mean the review is neutral. For simplicity we've combined scores **4,5** and tagged it as a positive score and combined scores **1,2** and tagged it as a negative score. Reviews with a score of **3** (neutal review) have not beeen incuded. The `Text` and `Summary` columns have been pre-processed. <a href=\"https://www.kaggle.com/snap/amazon-fine-food-reviews\">Here</a> is a link to the original dataset.\n",
    "\n",
    "We have provided a dataset with pre-preprocessed text and summary column. To obtain the sentence vector for our model, we'll be focusing on just the `Text` column. In the `Score` column a score of 1 indicates a positive review and a score of 0 indicates a negative review. The dataset is highly imbalanced with ~88 % of reviews belonging to the positive class (representend with a 1 in the `Score` column) and ~12 % belonging to the negative class (represented with a 0 in the `Score` column).\n",
    "\n",
    "Run the below two cells to download the pre-processed train and test csv files. The dataset has been hosted on Google Drive and has been made available to anyone with the link. \n",
    "\n",
    "1. https://medium.com/@annissouames99/how-to-upload-files-automatically-to-drive-with-python-ee19bb13dda\n",
    "2. Link to `train.csv` : https://drive.google.com/open?id=1IyCOmYJFILPkhx6ASvI0iJ_3Obvw1g0O\n",
    "3. Link to `test.csv` : https://drive.google.com/open?id=1g0RoPXFUUgUGh4Mxa3QY_J_tz3LO_IqA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEZOXw2ppmEh"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-6beb6b456bb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleAuth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpydrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrive\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleDrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0moauth2client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGoogleCredentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Authenticate and create the PyDrive client.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C6F5-35fuDDl"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'drive' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-bea78512b0e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mid_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"1g0RoPXFUUgUGh4Mxa3QY_J_tz3LO_IqA\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf_train_downloaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCreateFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mid_train\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdf_train_downloaded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetContentFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'drive' is not defined"
     ]
    }
   ],
   "source": [
    "id_train = \"1IyCOmYJFILPkhx6ASvI0iJ_3Obvw1g0O\"\n",
    "id_test = \"1g0RoPXFUUgUGh4Mxa3QY_J_tz3LO_IqA\"\n",
    "\n",
    "df_train_downloaded = drive.CreateFile({'id':id_train}) \n",
    "df_train_downloaded.GetContentFile('train.csv')\n",
    "\n",
    "df_test_downloaded = drive.CreateFile({'id':id_test}) \n",
    "df_test_downloaded.GetContentFile('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNbFO_PwvV2l"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-9ec331af5bcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# You should have train.csv and test.csv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ls' is not defined"
     ]
    }
   ],
   "source": [
    "# You should have train.csv and test.csv\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxEYDnB5Sgjk"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZFsxePaCA97x"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>492223</td>\n",
       "      <td>492224</td>\n",
       "      <td>B0009TN7F2</td>\n",
       "      <td>ATOO55BX20RGF</td>\n",
       "      <td>J Riddle</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1300579200</td>\n",
       "      <td>good product convenience</td>\n",
       "      <td>good fresh product serves great service hand t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>501225</td>\n",
       "      <td>501226</td>\n",
       "      <td>B002F4XTAC</td>\n",
       "      <td>A3I7K7XBLOBHVD</td>\n",
       "      <td>M. Jones \"CChip\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1323216000</td>\n",
       "      <td>cookie</td>\n",
       "      <td>love cookies last order found five bags opened...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>170153</td>\n",
       "      <td>170154</td>\n",
       "      <td>B002ZOG29C</td>\n",
       "      <td>A1RHWDTPWDTTLC</td>\n",
       "      <td>W. P. \"nuttinbutdtruth\"</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1300665600</td>\n",
       "      <td>absolutely fabulous starbucks fans look elsewhere</td>\n",
       "      <td>without doubt one best coffee one poor review ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Id   ProductId          UserId              ProfileName  \\\n",
       "0      492223  492224  B0009TN7F2   ATOO55BX20RGF                 J Riddle   \n",
       "1      501225  501226  B002F4XTAC  A3I7K7XBLOBHVD         M. Jones \"CChip\"   \n",
       "2      170153  170154  B002ZOG29C  A1RHWDTPWDTTLC  W. P. \"nuttinbutdtruth\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       3      1  1300579200   \n",
       "1                     0                       0      1  1323216000   \n",
       "2                    11                      11      1  1300665600   \n",
       "\n",
       "                                             Summary  \\\n",
       "0                           good product convenience   \n",
       "1                                             cookie   \n",
       "2  absolutely fabulous starbucks fans look elsewhere   \n",
       "\n",
       "                                                Text  \n",
       "0  good fresh product serves great service hand t...  \n",
       "1  love cookies last order found five bags opened...  \n",
       "2  without doubt one best coffee one poor review ...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272184, 11)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01WhECmySgjs"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102592</td>\n",
       "      <td>102593</td>\n",
       "      <td>B000BKXUXS</td>\n",
       "      <td>A13CLGTCDY7425</td>\n",
       "      <td>Todd A. Molloy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1348099200</td>\n",
       "      <td>wooooooooooo</td>\n",
       "      <td>created head monster mints incredible worthy m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>483050</td>\n",
       "      <td>483051</td>\n",
       "      <td>B0012KIB8K</td>\n",
       "      <td>A1K0BHXEOWXVZ3</td>\n",
       "      <td>Tim Gulley</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1237766400</td>\n",
       "      <td>cats love em</td>\n",
       "      <td>two kitties love food pouches seem enjoy flavo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>458853</td>\n",
       "      <td>458854</td>\n",
       "      <td>B001EQ4RU8</td>\n",
       "      <td>A3A0G5OX8OERGA</td>\n",
       "      <td>Chua Keh Soon \"alan\"</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1285891200</td>\n",
       "      <td>trust marking around</td>\n",
       "      <td>far problems love shop amazon com br br singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      Id   ProductId          UserId           ProfileName  \\\n",
       "0      102592  102593  B000BKXUXS  A13CLGTCDY7425        Todd A. Molloy   \n",
       "1      483050  483051  B0012KIB8K  A1K0BHXEOWXVZ3            Tim Gulley   \n",
       "2      458853  458854  B001EQ4RU8  A3A0G5OX8OERGA  Chua Keh Soon \"alan\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     0                       0      1  1348099200   \n",
       "1                    11                      13      1  1237766400   \n",
       "2                     1                       2      1  1285891200   \n",
       "\n",
       "                Summary                                               Text  \n",
       "0          wooooooooooo  created head monster mints incredible worthy m...  \n",
       "1          cats love em  two kitties love food pouches seem enjoy flavo...  \n",
       "2  trust marking around  far problems love shop amazon com br br singapore  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68046, 11)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7B_E8e2A98D"
   },
   "source": [
    "### Modeling\n",
    "\n",
    "We will be using logistic regression to build a baseline model. Logistic regression is a linear model for classification and a good option for building our baseline. We'll be using sklearn module to build our model. We also use the grid search method to find the best hyperparameters for the logistic regression model and observe the model performance. <a href=\"https://ayearofai.com/rohan-6-follow-up-statistical-interpretation-of-logistic-regression-e78de3b4d938\">Here</a> is a small refresher on logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbDJc69gSgjx"
   },
   "outputs": [],
   "source": [
    "training_data_size = 20000  # you can experiment with more data to get a more realistic performance score. With a fewer datapoints the model tends to overfit\n",
    "\n",
    "text_train = df_train['Text'].iloc[0:training_data_size].values\n",
    "y_train = df_train['Score'].iloc[0:training_data_size].values\n",
    "\n",
    "text_test = df_train['Text'].iloc[0:training_data_size].values\n",
    "y_test = df_train['Score'].iloc[0:training_data_size].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pAcqzVTSA98N"
   },
   "source": [
    "#### 1. BERT last layer hidden state as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-lnbua8SgkJ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 20000/20000 [16:18<00:00, 18.60it/s]\n",
      "100%|████████████████████████████████████| 20000/20000 [16:28<00:00, 20.24it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "for sentence in tqdm(text_train):\n",
    "    x_train.append(embed_model.sentence2vec(sentence, layers=embed_model.LAST_LAYER))\n",
    "    \n",
    "x_test = []\n",
    "for sentence in tqdm(text_test):\n",
    "    x_test.append(embed_model.sentence2vec(sentence, layers=embed_model.LAST_LAYER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1D1Z9y6pSgkO"
   },
   "outputs": [],
   "source": [
    "parameters = {'loss' :['log'],'penalty':['l1','l2','elasticnet'],'alpha':[float(i)/10000000 for i in range(1,5,1)],'n_jobs':[-1]}\n",
    "LR = SGDClassifier(fit_intercept=True, random_state=42)\n",
    "scoring = 'recall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sIcu3EMPA98Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params grid search {'alpha': 1e-07, 'loss': 'log', 'n_jobs': -1, 'penalty': 'elasticnet'}\n",
      "Precision 98.89902797064074\n",
      "Recall 58.96162261250074\n",
      "f1 score 73.87841292186863\n"
     ]
    }
   ],
   "source": [
    "clf = model_grid_search(parameters, x_train, y_train, model=LR, scoring=scoring)\n",
    "print(\"Best params grid search\", clf.best_params_)\n",
    "y_pred = clf.predict(x_test)\n",
    "metrics_dict = test_metrics(y_pred, y_test)\n",
    "print(\"Precision\", metrics_dict['precision'])\n",
    "print(\"Recall\", metrics_dict['recall'])\n",
    "print(\"f1 score\", metrics_dict['F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoVl36GYA98U",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(metrics_dict['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXGvw1viSgj1"
   },
   "source": [
    "#### 2.  BERT last 4 layer hidden states as embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nf9d4Uf2A98E",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 20000/20000 [28:29<00:00,  9.00it/s]\n",
      "100%|████████████████████████████████████| 20000/20000 [29:11<00:00,  9.46it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "for sentence in tqdm(text_train):\n",
    "    x_train.append(embed_model.sentence2vec(sentence, layers=embed_model.LAST_4_LAYERS))\n",
    "    \n",
    "x_test = []\n",
    "for sentence in tqdm(text_test):\n",
    "    x_test.append(embed_model.sentence2vec(sentence, layers=embed_model.LAST_4_LAYERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOeg2ZjiSgj5"
   },
   "outputs": [],
   "source": [
    "parameters = {'loss' :['log'],'penalty':['l1','l2','elasticnet'],'alpha':[float(i)/10000000 for i in range(1,5,1)],'n_jobs':[-1]}\n",
    "LR = SGDClassifier(fit_intercept=True, random_state=42)\n",
    "scoring = 'recall'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhI5OaeUA98G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params grid search {'alpha': 2e-07, 'loss': 'log', 'n_jobs': -1, 'penalty': 'l2'}\n",
      "Precision 97.86965441060438\n",
      "Recall 73.34870794157649\n",
      "f1 score 83.85330403920906\n"
     ]
    }
   ],
   "source": [
    "LR = SGDClassifier(fit_intercept=True)\n",
    "clf = model_grid_search(parameters, x_train, y_train, model=LR, scoring=scoring)\n",
    "print(\"Best params grid search\", clf.best_params_)\n",
    "y_pred = clf.predict(x_test)\n",
    "metrics_dict = test_metrics(y_pred, y_test)\n",
    "print(\"Precision\", metrics_dict['precision'])\n",
    "print(\"Recall\", metrics_dict['recall'])\n",
    "print(\"f1 score\", metrics_dict['F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ei1PsG2niiy8"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_confusion_matrix(metrics_dict['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZaMzno1SgkY"
   },
   "source": [
    "## Things to try:\n",
    "\n",
    "1. Try different non-linear classifiers (decision tree, random forest .etc) to achieve better results than the baseline.\n",
    "2. Try different ways of building your word embeddings and use that on your desired classification model: (refer to the previous section [above](https://colab.research.google.com/drive/1g7OWTJKVWwij9Bit9nZDamaUI2--fjmO#scrollTo=ytonVXQWGcei))\n",
    "    - Sum of all 12 layer hidden states (word embedding would be of size 768)\n",
    "    - First layer hidden state (word embedding would be of size 768)\n",
    "    - Second to last layer hidden state (word embedding would be of size 768)\n",
    "    - Concat of last 4 layer hidden states (word embedding would be of size 768 * 4)\n",
    "3. Use larger models such as `bert-large-uncased` instead of `bert-base-uncased`\n",
    "3. You can also use `BERT as service`, to obtain <a href=\"https://github.com/hanxiao/bert-as-service#getting-elmo-like-contextual-word-embedding\">sentence embeddings</a> (it supports many other features as well and their repo is pretty well documented) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1hnay4sEA98Y"
   },
   "source": [
    "## References:\n",
    "1. https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/\n",
    "2. http://jalammar.github.io/illustrated-bert/\n",
    "3. https://jalammar.github.io/illustrated-transformer/\n",
    "4. https://towardsdatascience.com/nlp-extract-contextualized-embeddings-from-bert-keras-tf-67ef29f60a7b\n",
    "5. [Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html): a blogpost by Professor Sasha Rush describing the transformer architecture by implementing it from the paper in PyTorch."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of BERT_lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
